{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "colab": {
      "name": "preprocess_review",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ozj8ZdU3xC4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!pip install pytorch_transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5ELz6xPxBgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import torch\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from pytorch_transformers import BertTokenizer, BertModel\n",
        "\n",
        "import re\n",
        "\n",
        "df = pd.read_csv('/content/gdrive/My Drive/IMDB_Dataset.csv', encoding='utf-8')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqzReP2yxBgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sentiment 1, 0으로 바꾸기\n",
        "df['sentiment'] = (df['sentiment'] == 'positive').astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEgV8AqnxBgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 문장 단위로 쪼개기\n",
        "df.review = df.review.map(sent_tokenize)\n",
        "df_reviews = df_reviews.stack().reset_index(level=1, drop=True).to_frame('review_single')\n",
        "df_pre = df[[\"sentiment\"]].merge(df_reviews, left_index=True, right_index=True, how='left')\n",
        "raw_reviews = df_pre.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fu-0Mgq4FKRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 클렌징\n",
        "stop = stopwords.words('english')\n",
        "lmtzr = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def clean_review(raw_review: str) -> str:\n",
        "    # 1. unicode to ASCII\n",
        "    raw_review = unicodeToAscii(raw_review)\n",
        "    \n",
        "    # 2. Remove HTML\n",
        "    review_text = BeautifulSoup(raw_review, \"lxml\").get_text()\n",
        "    \n",
        "    # 3. Tokenize:\n",
        "    review_text = [word for word in nltk.word_tokenize(review_text)]\n",
        "    \n",
        "    # 4. Remove Stop Words\n",
        "    review_text = [word for word in review_text if word not in stop and len(word) >= 3]\n",
        "    \n",
        "    # 5. lemmatization\n",
        "    review_text = [lmtzr.lemmatize(word) for word in review_text]\n",
        "\n",
        "    return \" \".join(review_text)\n",
        "  \n",
        "reviews = []\n",
        "  \n",
        "for r in raw_reviews:\n",
        "  new_r = clean_review(r[1])\n",
        "  if(len(new_r) > 5 and new_r[0].isalpha()):\n",
        "    reviews.append((new_r, r[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N7Dv9PrXUrY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import gzip\n",
        "\n",
        "with gzip.open('/content/gdrive/My Drive/pre_reviews.pickle', 'wb') as f:\n",
        "    pickle.dump(reviews, f)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}