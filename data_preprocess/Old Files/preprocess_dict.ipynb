{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "colab": {
      "name": "preprocess_dict",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ozj8ZdU3xC4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!pip install pytorch_transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5ELz6xPxBgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import torch\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from pytorch_transformers import BertTokenizer, BertModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfgR0RrTYf3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import gzip\n",
        "\n",
        "with gzip.open('/content/gdrive/My Drive/pre_reviews.pickle', 'rb') as f:\n",
        "    reviews = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvyb2hKHxBgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcrn2wHRxqV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import gzip\n",
        "\n",
        "with gzip.open('/content/gdrive/My Drive/Top5000_BtoA.pickle','rb') as f:\n",
        "    Top5000_BtoA = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44Qkj5jBxBgq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BERT\n",
        "def preprocess(review: str, total: int, show_progress: bool = True) -> list:\n",
        "    if show_progress:\n",
        "        global counter\n",
        "        counter += 1\n",
        "        if counter % 10000 == 0:\n",
        "          print('Processing... %6i/%6i'% (counter, total))\n",
        "        \n",
        "    token_ids = tokenizer.encode(review, add_special_tokens=True)\n",
        "    \n",
        "    for token_id in token_ids:\n",
        "        try:\n",
        "            dictionary[token_id] += 1\n",
        "        except:\n",
        "            dictionary[token_id] = 1\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvnAX73SxBgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "counter = 0\n",
        "count = 0\n",
        "\n",
        "dictionary = {}\n",
        "\n",
        "batch_size = 512\n",
        "num = 1\n",
        "\n",
        "length = len(reviews)\n",
        "for review in reviews:\n",
        "  preprocess(review[0], length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81kZddHCGRUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = [(dictionary[i], i) for i in dictionary.keys()]\n",
        "x.sort(reverse = True)\n",
        "\n",
        "Top5000 = [x[i][1] for i in range(5000)]\n",
        "Top5000_dist = {x[i][1] : x[i][0] for i in range(5000)}\n",
        "Top5000_BtoA = {Top5000[i] : i for i in range(5000)}\n",
        "Top5000_AtoB = {i : Top5000[i] for i in range(5000)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMa65bdZbiF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import gzip\n",
        "\n",
        "with gzip.open('Top5000_dist.pickle', 'wb') as f:\n",
        "    pickle.dump(Top5000_dist, f)\n",
        "    \n",
        "with gzip.open('Top5000_BtoA.pickle', 'wb') as f:\n",
        "    pickle.dump(Top5000_BtoA, f)\n",
        "\n",
        "with gzip.open('Top5000_AtoB.pickle', 'wb') as f:\n",
        "    pickle.dump(Top5000_AtoB, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW-SqAYXbjWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}